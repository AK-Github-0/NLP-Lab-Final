{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1azvPAU4STXAicRt26DY5wGPIX660da1-",
      "authorship_tag": "ABX9TyOAGSk4gIbPUwQOssn4kl2H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AK-Github-0/NLP-Lab-Final/blob/main/2020024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries and Modules"
      ],
      "metadata": {
        "id": "mYXycLe3PXGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "import plotly.offline as pyo\n",
        "from plotly.subplots import make_subplots\n",
        "import re\n",
        "import string \n",
        "import nltk\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from wordcloud import WordCloud\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "JO5N2yFsPdhw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rAgXUfrTy9U",
        "outputId": "5ac99d71-6fd5-48be-ad51-318c3f057d3e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining all functions"
      ],
      "metadata": {
        "id": "Sr_Q5dEiQrrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def load_dataset(path):\n",
        "  df = pd.read_csv(path)\n",
        "  df = df[:100]\n",
        "  return df\n",
        "\n"
      ],
      "metadata": {
        "id": "Tidbi1YMP6Aw"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def EDA(df):\n",
        "  plt.figure(figsize = (12,6))\n",
        "  sns.histplot(df['user_verified'])\n",
        "  plt.title('Account Distribution',fontsize = 20)\n",
        "  plt.savefig('user_verifiedornot.png')\n",
        "\n",
        "\n",
        "  data_=df['source'].value_counts().reset_index()\n",
        "\n",
        "  trace1=go.Bar(x=['Twitter Web App', 'Twitter for iPhone', 'Twitter for Android','LinkedIn', 'HubSpot', 'IFTTT', \n",
        "                 'Hypefury', 'Sprout Social','Revive Social App', 'Buffer'],\n",
        "              y=data_['source'],marker=dict(color='rgb(250,13,92)',\n",
        "              line=dict(color='rgb(0,0,0)',width=1.5)),text=data_['source'],textposition='outside')\n",
        "\n",
        "  layout=go.Layout(template='plotly_dark',title='Top 10 Most Source Disrtibution Of Tweets',xaxis=dict(title='Source'),\n",
        "                 yaxis=dict(title='Count'),height=700)\n",
        "  fig=go.Figure(data=[trace1],layout=layout)\n",
        "  fig.write_image(\"Source_distribution.jpeg\")\n",
        "\n",
        "  data_verified=df[df['user_verified']==True].reset_index()\n",
        "  data_not_verified=df[df['user_verified']==False].reset_index()\n",
        "\n",
        "  all_hashtags=[]\n",
        "  for i in range(len(data_verified['hashtags'])):\n",
        "      a=data_verified['hashtags'][i].strip('][').split(', ')\n",
        "      for i in a:\n",
        "          all_hashtags.append(i)\n",
        "  all_hashtags=pd.Series(np.array(all_hashtags))\n",
        "  common_hashtags=all_hashtags.value_counts()[:30].rename_axis('common hashtags').reset_index(name='count')\n",
        "  fig=px.treemap(common_hashtags,path=['common hashtags'],values='count',title='30 Most common hashtags by Verified Accounts')\n",
        "  fig.write_image(\"Common_hashtags_by_verified.jpeg\")\n",
        "\n",
        "  all_hashtags=[]\n",
        "  for i in range(len(data_verified['hashtags'])):\n",
        "      a=data_not_verified['hashtags'][i].strip('][').split(', ')\n",
        "      for i in a:\n",
        "          all_hashtags.append(i)\n",
        "  all_hashtags=pd.Series(np.array(all_hashtags))\n",
        "  common_hashtags=all_hashtags.value_counts()[:30].rename_axis('common hashtags').reset_index(name='count')\n",
        "  fig=px.treemap(common_hashtags,path=['common hashtags'],values='count',title='30 Most common hashtags by unverified Accounts')\n",
        "  fig.write_image(\"Common_hashtags_by_unverified.jpeg\")\n",
        "\n",
        "  data_=data_verified['source'].value_counts().reset_index()\n",
        "  trace1=go.Bar(x=['Twitter Web App', 'Twitter for iPhone', 'Twitter for Android','LinkedIn', 'HubSpot', 'IFTTT', \n",
        "                  'Hypefury', 'Sprout Social','Revive Social App', 'Buffer',],y=data_['source'],\n",
        "              marker=dict(color='rgb(250,13,92)',line=dict(color='rgb(0,0,0)',width=1.5)),text=data_['source'],\n",
        "              textposition='outside')\n",
        "  layout=go.Layout(template='plotly_dark',title='Top 20 Most Source Distribution of Tweets From Verified Accounts',xaxis=dict(title='Source'),\n",
        "                  yaxis=dict(title='Count'),height=650)\n",
        "  fig=go.Figure(data=[trace1],layout=layout)\n",
        "  fig.write_image(\"Source_distributions_from_verified.jpeg\")\n",
        "\n"
      ],
      "metadata": {
        "id": "7fLoJvOTa5_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def remove_line_breaks(text):\n",
        "    text = text.replace('\\r', ' ').replace('\\n', ' ')\n",
        "    return text\n",
        "\n",
        "    \n",
        "def remove_punctuation(text):\n",
        "    re_replacements = re.compile(\"__[A-Z]+__\")  # such as __NAME__, __LINK__\n",
        "    re_punctuation = re.compile(\"[%s]\" % re.escape(string.punctuation))\n",
        "    '''Escape all the characters in pattern except ASCII letters and numbers'''\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_zero_punctuation = []\n",
        "    for token in tokens:\n",
        "        if not re_replacements.match(token):\n",
        "            token = re_punctuation.sub(\" \", token)\n",
        "        tokens_zero_punctuation.append(token)\n",
        "    return ' '.join(tokens_zero_punctuation)\n",
        "\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "def lowercase(text):\n",
        "    text_low = [token.lower() for token in word_tokenize(text)]\n",
        "    return ' '.join(text_low)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop = set(stopwords.words('english'))\n",
        "    word_tokens = nltk.word_tokenize(text)\n",
        "    text = \" \".join([word for word in word_tokens if word not in stop])\n",
        "    return text\n",
        "\n",
        "def remove_one_character_words(text):\n",
        "    '''Remove words from dataset that contain only 1 character'''\n",
        "    text_high_use = [token for token in word_tokenize(text) if len(token)>1]      \n",
        "    return ' '.join(text_high_use)   \n",
        "    \n",
        "#%%\n",
        "# Stemming with 'Snowball stemmer\" package\n",
        "def stem(text):\n",
        "    stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
        "    text_stemmed = [stemmer.stem(token) for token in word_tokenize(text)]        \n",
        "    return ' '.join(text_stemmed)\n",
        "\n",
        "def lemma(text):\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    word_tokens = nltk.word_tokenize(text)\n",
        "    text_lemma = \" \".join([wordnet_lemmatizer.lemmatize(word) for word in word_tokens])       \n",
        "    return ' '.join(text_lemma)\n",
        "\n",
        "def sentence_word(text):\n",
        "    word_tokens = nltk.word_tokenize(text)\n",
        "    return word_tokens\n",
        "#break paragraphs to sentence token \n",
        "def paragraph_sentence(text):\n",
        "    sent_token = nltk.sent_tokenize(text)\n",
        "    return sent_token    \n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"Return a list of words in a text.\"\"\"\n",
        "    return re.findall(r'\\w+', text)\n",
        "\n",
        "def remove_numbers(text):\n",
        "    no_nums = re.sub(r'\\d+', '', text)\n",
        "    return ''.join(no_nums)\n",
        "\n",
        "def clean_text(text):\n",
        "    _steps = [\n",
        "    remove_line_breaks,\n",
        "    remove_one_character_words,\n",
        "    remove_special_characters,\n",
        "    lowercase,\n",
        "    remove_punctuation,\n",
        "    remove_stopwords,\n",
        "    stem,\n",
        "    remove_numbers\n",
        "]\n",
        "    for step in _steps:\n",
        "        text=step(text)\n",
        "    return text   \n",
        "\n"
      ],
      "metadata": {
        "id": "MZ4jTRReScm6"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "huN4mrtzVWFj"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uHasfR9bU9o9"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "78bqtw2BVZJW"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NaCiXdAMVqEL"
      },
      "execution_count": 51,
      "outputs": []
    }
  ]
}