{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1azvPAU4STXAicRt26DY5wGPIX660da1-",
      "authorship_tag": "ABX9TyOMNLuD3WsQrGbh15dN1bwn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AK-Github-0/NLP-Lab-Final/blob/main/2020024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries and Modules"
      ],
      "metadata": {
        "id": "mYXycLe3PXGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "import plotly.offline as pyo\n",
        "from plotly.subplots import make_subplots\n",
        "import re\n",
        "import string \n",
        "import nltk\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from wordcloud import WordCloud\n",
        "from tqdm.auto import tqdm\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from textblob import TextBlob\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JO5N2yFsPdhw",
        "outputId": "dd41e4df-7290-41f4-8d80-91cb060250fd"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining all functions"
      ],
      "metadata": {
        "id": "Sr_Q5dEiQrrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def load_dataset(path):\n",
        "  df = pd.read_csv(path)\n",
        "  df = df[:100]\n",
        "  return df\n",
        "\n"
      ],
      "metadata": {
        "id": "Tidbi1YMP6Aw"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def EDA(df):\n",
        "  plt.figure(figsize = (12,6))\n",
        "  sns.histplot(df['user_verified'])\n",
        "  plt.title('Account Distribution',fontsize = 20)\n",
        "  plt.savefig('user_verifiedornot.png')\n",
        "\n",
        "\n",
        "  data_=df['source'].value_counts().reset_index()\n",
        "\n",
        "  trace1=go.Bar(x=['Twitter Web App', 'Twitter for iPhone', 'Twitter for Android','LinkedIn', 'HubSpot', 'IFTTT', \n",
        "                 'Hypefury', 'Sprout Social','Revive Social App', 'Buffer'],\n",
        "              y=data_['source'],marker=dict(color='rgb(250,13,92)',\n",
        "              line=dict(color='rgb(0,0,0)',width=1.5)),text=data_['source'],textposition='outside')\n",
        "\n",
        "  layout=go.Layout(template='plotly_dark',title='Top 10 Most Source Disrtibution Of Tweets',xaxis=dict(title='Source'),\n",
        "                 yaxis=dict(title='Count'),height=700)\n",
        "  fig=go.Figure(data=[trace1],layout=layout)\n",
        "  fig.write_image(\"Source_distribution.jpeg\")\n",
        "\n",
        "  data_verified=df[df['user_verified']==True].reset_index()\n",
        "  data_not_verified=df[df['user_verified']==False].reset_index()\n",
        "\n",
        "  all_hashtags=[]\n",
        "  for i in range(len(data_verified['hashtags'])):\n",
        "      a=data_verified['hashtags'][i].strip('][').split(', ')\n",
        "      for i in a:\n",
        "          all_hashtags.append(i)\n",
        "  all_hashtags=pd.Series(np.array(all_hashtags))\n",
        "  common_hashtags=all_hashtags.value_counts()[:30].rename_axis('common hashtags').reset_index(name='count')\n",
        "  fig=px.treemap(common_hashtags,path=['common hashtags'],values='count',title='30 Most common hashtags by Verified Accounts')\n",
        "  fig.write_image(\"Common_hashtags_by_verified.jpeg\")\n",
        "\n",
        "  all_hashtags=[]\n",
        "  for i in range(len(data_verified['hashtags'])):\n",
        "      a=data_not_verified['hashtags'][i].strip('][').split(', ')\n",
        "      for i in a:\n",
        "          all_hashtags.append(i)\n",
        "  all_hashtags=pd.Series(np.array(all_hashtags))\n",
        "  common_hashtags=all_hashtags.value_counts()[:30].rename_axis('common hashtags').reset_index(name='count')\n",
        "  fig=px.treemap(common_hashtags,path=['common hashtags'],values='count',title='30 Most common hashtags by unverified Accounts')\n",
        "  fig.write_image(\"Common_hashtags_by_unverified.jpeg\")\n",
        "\n",
        "  data_=data_verified['source'].value_counts().reset_index()\n",
        "  trace1=go.Bar(x=['Twitter Web App', 'Twitter for iPhone', 'Twitter for Android','LinkedIn', 'HubSpot', 'IFTTT', \n",
        "                  'Hypefury', 'Sprout Social','Revive Social App', 'Buffer',],y=data_['source'],\n",
        "              marker=dict(color='rgb(250,13,92)',line=dict(color='rgb(0,0,0)',width=1.5)),text=data_['source'],\n",
        "              textposition='outside')\n",
        "  layout=go.Layout(template='plotly_dark',title='Top 20 Most Source Distribution of Tweets From Verified Accounts',xaxis=dict(title='Source'),\n",
        "                  yaxis=dict(title='Count'),height=650)\n",
        "  fig=go.Figure(data=[trace1],layout=layout)\n",
        "  fig.write_image(\"Source_distributions_from_verified.jpeg\")\n",
        "\n"
      ],
      "metadata": {
        "id": "7fLoJvOTa5_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def remove_line_breaks(text):\n",
        "    text = text.replace('\\r', ' ').replace('\\n', ' ')\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    re_replacements = re.compile(\"__[A-Z]+__\")  # such as __NAME__, __LINK__\n",
        "    re_punctuation = re.compile(\"[%s]\" % re.escape(string.punctuation))\n",
        "    '''Escape all the characters in pattern except ASCII letters and numbers'''\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens_zero_punctuation = []\n",
        "    for token in tokens:\n",
        "        if not re_replacements.match(token):\n",
        "            token = re_punctuation.sub(\" \", token)\n",
        "        tokens_zero_punctuation.append(token)\n",
        "    return ' '.join(tokens_zero_punctuation)\n",
        "\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def lowercase(text):\n",
        "    text_low = [token.lower() for token in word_tokenize(text)]\n",
        "    return ' '.join(text_low)\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop = set(stopwords.words('english'))\n",
        "    word_tokens = nltk.word_tokenize(text)\n",
        "    text = \" \".join([word for word in word_tokens if word not in stop])\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_one_character_words(text):\n",
        "    '''Remove words from dataset that contain only 1 character'''\n",
        "    text_high_use = [token for token in word_tokenize(text) if len(token)>1]      \n",
        "    return ' '.join(text_high_use)   \n",
        "    \n",
        "\n",
        "def stem(text):\n",
        "    stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
        "    text_stemmed = [stemmer.stem(token) for token in word_tokenize(text)]        \n",
        "    return ' '.join(text_stemmed)\n",
        "\n",
        "def lemma(text):\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    word_tokens = nltk.word_tokenize(text)\n",
        "    text_lemma = \" \".join([wordnet_lemmatizer.lemmatize(word) for word in word_tokens])       \n",
        "    return ' '.join(text_lemma)\n",
        "\n",
        "def sentence_word(text):\n",
        "    word_tokens = nltk.word_tokenize(text)\n",
        "    return word_tokens\n",
        "\n",
        "def paragraph_sentence(text):\n",
        "    sent_token = nltk.sent_tokenize(text)\n",
        "    return sent_token    \n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"Return a list of words in a text.\"\"\"\n",
        "    return re.findall(r'\\w+', text)\n",
        "\n",
        "def remove_numbers(text):\n",
        "    no_nums = re.sub(r'\\d+', '', text)\n",
        "    return ''.join(no_nums)\n",
        "\n",
        "def clean_text(text):\n",
        "    _steps = [\n",
        "    remove_line_breaks,\n",
        "    remove_one_character_words,\n",
        "    remove_special_characters,\n",
        "    lowercase,\n",
        "    remove_punctuation,\n",
        "    remove_stopwords,\n",
        "    stem,\n",
        "    remove_numbers\n",
        "]\n",
        "    for step in _steps:\n",
        "        text=step(text)\n",
        "    return text   \n",
        "\n"
      ],
      "metadata": {
        "id": "MZ4jTRReScm6"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DF2TXT(df):\n",
        "  text = ' '\n",
        "  for x in df['text']:\n",
        "    text = text + x\n",
        "  return text"
      ],
      "metadata": {
        "id": "2RIlAatunn5m"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NGramAnalysis(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  stop_words = list(stopwords.words('english'))\n",
        "  clean_word_data = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "  bigrams_list = [\"_\".join(item) for item in nltk.bigrams(clean_word_data)]\n",
        "  trigrams_list = [\"_\".join(item) for item in nltk.trigrams(clean_word_data)]\n",
        "  bigram_counts = Counter(bigrams_list)\n",
        "  trigram_counts = Counter(trigrams_list)\n",
        "    \n",
        "\n",
        "  top_10_bigrams = bigram_counts.most_common(10)\n",
        "  top_10_trigrams = trigram_counts.most_common(10)\n",
        "\n",
        "\n",
        "  bigram_labels, bigram_values = zip(*top_10_bigrams)\n",
        "  trigram_labels, trigram_values = zip(*top_10_trigrams)\n",
        "\n",
        "\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.subplot(121)\n",
        "  plt.barh(range(len(bigram_labels)), bigram_values, align='center')\n",
        "  plt.yticks(range(len(bigram_labels)), bigram_labels)\n",
        "  plt.xlabel('Count')\n",
        "  plt.ylabel('Bigrams')\n",
        "  plt.title('Top 10 Most Common Bigrams')\n",
        "\n",
        "  plt.subplot(122)\n",
        "  plt.barh(range(len(trigram_labels)), trigram_values, align='center')\n",
        "  plt.yticks(range(len(trigram_labels)), trigram_labels)\n",
        "  plt.xlabel('Count')\n",
        "  plt.ylabel('Trigrams')\n",
        "  plt.title('Top 10 Most Common Trigrams')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.savefig('TrigramsBigrams.png')\n",
        "\n",
        "\n",
        "def show_wordcloud(df):\n",
        "  text = \" \".join(cat.split()[1] for cat in df.text)\n",
        "  \n",
        "  word_cloud = WordCloud(collocations = False, background_color = 'white').generate(text)\n",
        "\n",
        "  plt.imshow(word_cloud, interpolation='bilinear')\n",
        "  plt.axis(\"off\")\n",
        "  plt.savefig('WordCloud.png')"
      ],
      "metadata": {
        "id": "akUwVdEgmJJc"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_textblob_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    sentiment = blob.sentiment.polarity\n",
        "    if sentiment > 0:\n",
        "        return 'Positive'\n",
        "    elif sentiment < 0:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'"
      ],
      "metadata": {
        "id": "StVi9xBrtLQH"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentimentanalysis(df):\n",
        "  df['preprocessed_text'] = df['text'].apply(clean_text)\n",
        "  df['textblob_sentiment'] = df['preprocessed_text'].apply(get_textblob_sentiment)\n"
      ],
      "metadata": {
        "id": "TMQgxaYXq74t"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def Feature_Engineering(df):\n",
        "  text = df['text']\n",
        "  df['sentencelen'] = text.apply(lambda x: len(x.split('.')))\n",
        "  df['countofwords'] = text.apply(lambda x: len(x.split()))\n",
        "  df['spaces'] = text.apply(lambda x: x.count(' '))\n",
        "  df['characters'] = text.apply(len)\n",
        "  return df"
      ],
      "metadata": {
        "id": "7DGdh-fuzAOe"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def vectorization(df):\n",
        "  count_vectorizer = CountVectorizer()\n",
        "  tfidf_vectorizer = TfidfVectorizer()\n",
        "  sentences = [text.split() for text in df['text']]\n",
        "  w2v_model = Word2Vec(sentences, size=100, window=5, min_count=1, workers=4)\n",
        "  count_vectors = count_vectorizer.fit_transform(df['text'])\n",
        "  tfidf_vectors = tfidf_vectorizer.fit_transform(df['text'])\n",
        "  df['CountVect_df'] = count_vectors.toarray().tolist()\n",
        "  df['TfIDF_df'] = tfidf_vectors.toarray().tolist()\n",
        "  w2v_vectors = [sum([w2v_model.wv[word] for word in text.split() if word in w2v_model.wv])\n",
        "                   for text in df['text']]\n",
        "  df['Word2Vec_df'] = w2v_vectors\n",
        "  return df"
      ],
      "metadata": {
        "id": "8nvyes3v0Brj"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(df):\n",
        "  # Split the data into training and testing sets\n",
        "  X_train, X_test, y_train, y_test = train_test_split(df['preprocessed_text'], df['textblob_sentiment'], test_size=0.3, random_state=42)\n",
        "  # Vectorize the text data using TF-IDF\n",
        "  tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "  X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "  X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "  models = [\n",
        "    (\"LinearSVC\", LinearSVC()),\n",
        "    (\"Logistic Regression\", LogisticRegression(max_iter=1000)),\n",
        "    (\"MultinomialNB\", MultinomialNB()),\n",
        "    (\"Random Forest\", RandomForestClassifier(n_estimators=100)),\n",
        "    (\"Decision Tree\", DecisionTreeClassifier())\n",
        "  ]"
      ],
      "metadata": {
        "id": "K8BQNpCS6SQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_features_countvect_textblob(text):\n",
        "    blob = TextBlob(text)\n",
        "    sentiment = blob.sentiment.polarity\n",
        "    return [text, sentiment]\n",
        "\n",
        "df['features'] = df['preprocessed_text'].apply(extract_features_countvect_textblob)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['features'], df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "count_vect = CountVectorizer()\n",
        "\n",
        "X_train_counts = count_vect.fit_transform(X_train.apply(lambda x: x[0]))\n",
        "X_test_counts = count_vect.transform(X_test.apply(lambda x: x[0]))\n",
        "\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "\n",
        "X_train_tfidf = tfidf_vect.fit_transform(X_train.apply(lambda x: x[0]))\n",
        "X_test_tfidf = tfidf_vect.transform(X_test.apply(lambda x: x[0]))\n",
        "\n",
        "svc_model = LinearSVC()\n",
        "\n",
        "svc_model.fit(X_train_counts, y_train)\n",
        "y_pred = svc_model.predict(X_test_counts)\n",
        "print(\"Results for CountVectorizer features:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted')}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted')}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred, average='weighted')}\")\n",
        "\n",
        "svc_model.fit(X_train_tfidf, y_train)\n",
        "y_pred = svc_model.predict(X_test_tfidf)\n",
        "print(\"\\nResults for TF-IDF features:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted')}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted')}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred, average='weighted')}\")"
      ],
      "metadata": {
        "id": "4AfiPFSt6vlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Program"
      ],
      "metadata": {
        "id": "mCirOA2L64Ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentimentanalysis(df)"
      ],
      "metadata": {
        "id": "HWtZAFPXtWF3"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "oE-2tOR7vDiV",
        "outputId": "15589d3e-a60e-4580-a4e2-f04110e7d450"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         user_name                                               text  \\\n",
              "0           Bohmle  #GPT4 for FREE. \\nNo its not a clickbait, @Qol...   \n",
              "1              NaN                                      AI enthusiast   \n",
              "2     Dan Bruno AI  ChatGPT Thinks These 5 Crypto Coins Will Explo...   \n",
              "3  Georgiana Comsa  New: @JWVance's post about 5 #startups (includ...   \n",
              "4     Bitone Great  ðŸš¨Get Out!ðŸš¨\\nðŸ’°#Binance SpotðŸ’°\\nâ¬‡ Recommendation:...   \n",
              "\n",
              "               user_location  \\\n",
              "0                  Carkingga   \n",
              "1  2019-07-03 03:44:41+00:00   \n",
              "2             Manchester, NH   \n",
              "3                  Palo Alto   \n",
              "4                  Hong Kong   \n",
              "\n",
              "                                    user_description  \\\n",
              "0                                                NaN   \n",
              "1                                               60.0   \n",
              "2  The latest in #ChatGPT, #BARD, #Bing, and othe...   \n",
              "3  Founder of Silicon Valley PR, award-winning PR...   \n",
              "4  #ChatGPT (AI) powered Free Trading Signal! \\nL...   \n",
              "\n",
              "                user_created user_followers user_friends  \\\n",
              "0                        NaN            NaN          NaN   \n",
              "1                      349.0            611        False   \n",
              "2  2021-05-19 01:19:32+00:00          470.0        157.0   \n",
              "3  2008-12-24 09:32:23+00:00         3864.0       1883.0   \n",
              "4  2022-11-21 04:42:18+00:00         1517.0        506.0   \n",
              "\n",
              "             user_favourites user_verified                       date  \\\n",
              "0                        NaN           NaN                        NaN   \n",
              "1  2023-05-17 18:11:12+00:00         False            Twitter Web App   \n",
              "2                       5185         False  2023-05-17 18:11:03+00:00   \n",
              "3                       2415         False  2023-05-17 18:10:25+00:00   \n",
              "4                         64         False  2023-05-17 18:09:39+00:00   \n",
              "\n",
              "                           hashtags           source textblob_sentiment  \\\n",
              "0                               NaN              NaN           Positive   \n",
              "1                               NaN              NaN            Neutral   \n",
              "2       ['chatgpt', 'AI', 'openAI']          dlvr.it            Neutral   \n",
              "3         ['startups', 'startup50']  Twitter Web App           Negative   \n",
              "4  ['Binance', 'Short', 'GHSTUSDT']             rsi1           Negative   \n",
              "\n",
              "                                   preprocessed_text  \n",
              "0  gpt free clickbait qolaba studio chatbot power...  \n",
              "1                                      ai enthusiast  \n",
              "2  chatgpt think crypto coin explod year yahoo fi...  \n",
              "3  new jwvanc post startup includ vcinityinc st s...  \n",
              "4  get binanc spot recommend short ticker ghstusd...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9c325303-779c-4c14-ab2c-07f625602bbe\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_name</th>\n",
              "      <th>text</th>\n",
              "      <th>user_location</th>\n",
              "      <th>user_description</th>\n",
              "      <th>user_created</th>\n",
              "      <th>user_followers</th>\n",
              "      <th>user_friends</th>\n",
              "      <th>user_favourites</th>\n",
              "      <th>user_verified</th>\n",
              "      <th>date</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>source</th>\n",
              "      <th>textblob_sentiment</th>\n",
              "      <th>preprocessed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Bohmle</td>\n",
              "      <td>#GPT4 for FREE. \\nNo its not a clickbait, @Qol...</td>\n",
              "      <td>Carkingga</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Positive</td>\n",
              "      <td>gpt free clickbait qolaba studio chatbot power...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>AI enthusiast</td>\n",
              "      <td>2019-07-03 03:44:41+00:00</td>\n",
              "      <td>60.0</td>\n",
              "      <td>349.0</td>\n",
              "      <td>611</td>\n",
              "      <td>False</td>\n",
              "      <td>2023-05-17 18:11:12+00:00</td>\n",
              "      <td>False</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>ai enthusiast</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dan Bruno AI</td>\n",
              "      <td>ChatGPT Thinks These 5 Crypto Coins Will Explo...</td>\n",
              "      <td>Manchester, NH</td>\n",
              "      <td>The latest in #ChatGPT, #BARD, #Bing, and othe...</td>\n",
              "      <td>2021-05-19 01:19:32+00:00</td>\n",
              "      <td>470.0</td>\n",
              "      <td>157.0</td>\n",
              "      <td>5185</td>\n",
              "      <td>False</td>\n",
              "      <td>2023-05-17 18:11:03+00:00</td>\n",
              "      <td>['chatgpt', 'AI', 'openAI']</td>\n",
              "      <td>dlvr.it</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>chatgpt think crypto coin explod year yahoo fi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Georgiana Comsa</td>\n",
              "      <td>New: @JWVance's post about 5 #startups (includ...</td>\n",
              "      <td>Palo Alto</td>\n",
              "      <td>Founder of Silicon Valley PR, award-winning PR...</td>\n",
              "      <td>2008-12-24 09:32:23+00:00</td>\n",
              "      <td>3864.0</td>\n",
              "      <td>1883.0</td>\n",
              "      <td>2415</td>\n",
              "      <td>False</td>\n",
              "      <td>2023-05-17 18:10:25+00:00</td>\n",
              "      <td>['startups', 'startup50']</td>\n",
              "      <td>Twitter Web App</td>\n",
              "      <td>Negative</td>\n",
              "      <td>new jwvanc post startup includ vcinityinc st s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bitone Great</td>\n",
              "      <td>ðŸš¨Get Out!ðŸš¨\\nðŸ’°#Binance SpotðŸ’°\\nâ¬‡ Recommendation:...</td>\n",
              "      <td>Hong Kong</td>\n",
              "      <td>#ChatGPT (AI) powered Free Trading Signal! \\nL...</td>\n",
              "      <td>2022-11-21 04:42:18+00:00</td>\n",
              "      <td>1517.0</td>\n",
              "      <td>506.0</td>\n",
              "      <td>64</td>\n",
              "      <td>False</td>\n",
              "      <td>2023-05-17 18:09:39+00:00</td>\n",
              "      <td>['Binance', 'Short', 'GHSTUSDT']</td>\n",
              "      <td>rsi1</td>\n",
              "      <td>Negative</td>\n",
              "      <td>get binanc spot recommend short ticker ghstusd...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c325303-779c-4c14-ab2c-07f625602bbe')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9c325303-779c-4c14-ab2c-07f625602bbe button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9c325303-779c-4c14-ab2c-07f625602bbe');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    }
  ]
}